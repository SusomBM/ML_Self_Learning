{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uOZe3UO68U-"
   },
   "source": [
    "# Order Delivery Time Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qd9e2-HF6_85"
   },
   "source": [
    "## Objectives\n",
    "The objective of this assignment is to build a regression model that predicts the delivery time for orders placed through Porter. The model will use various features such as the items ordered, the restaurant location, the order protocol, and the availability of delivery partners.\n",
    "\n",
    "The key goals are:\n",
    "- Predict the delivery time for an order based on multiple input features\n",
    "- Improve delivery time predictions to optimiae operational efficiency\n",
    "- Understand the key factors influencing delivery time to enhance the model's accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcC6tJ2p7F2p"
   },
   "source": [
    "## Data Pipeline\n",
    "The data pipeline for this assignment will involve the following steps:\n",
    "1. **Data Loading**\n",
    "2. **Data Preprocessing and Feature Engineering**\n",
    "3. **Exploratory Data Analysis**\n",
    "4. **Model Building**\n",
    "5. **Model Inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGOQI_f72jV1"
   },
   "source": [
    "## Data Understanding\n",
    "The dataset contains information on orders placed through Porter, with the following columns:\n",
    "\n",
    "| Field                     | Description                                                                                 |\n",
    "|---------------------------|---------------------------------------------------------------------------------------------|\n",
    "| market_id                 | Integer ID representing the market where the restaurant is located.                         |\n",
    "| created_at                | Timestamp when the order was placed.                                                        |\n",
    "| actual_delivery_time      | Timestamp when the order was delivered.                                                     |\n",
    "| store_primary_category    | Category of the restaurant (e.g., fast food, dine-in).                                      |\n",
    "| order_protocol            | Integer representing how the order was placed (e.g., via Porter, call to restaurant, etc.). |\n",
    "| total_items               | Total number of items in the order.                                                         |\n",
    "| subtotal                  | Final price of the order.                                                                   |\n",
    "| num_distinct_items        | Number of distinct items in the order.                                                      |\n",
    "| min_item_price            | Price of the cheapest item in the order.                                                    |\n",
    "| max_item_price            | Price of the most expensive item in the order.                                              |\n",
    "| total_onshift_dashers     | Number of delivery partners on duty when the order was placed.                              |\n",
    "| total_busy_dashers        | Number of delivery partners already occupied with other orders.                             |\n",
    "| total_outstanding_orders  | Number of orders pending fulfillment at the time of the order.                              |\n",
    "| distance                  | Total distance from the restaurant to the customer.                                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QoCQFDzHUWP"
   },
   "source": [
    "## **Importing Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jun9CeAc7QOw"
   },
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and analysis\n",
    "\n",
    "# Import Warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy version: 1.26.4\n",
      "pandas version: 2.2.2\n",
      "matplotlib version: 3.10.0\n",
      "seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "# Check versions\n",
    "print(\"numpy version:\", np.__version__)\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"matplotlib version:\", plt.matplotlib.__version__)\n",
    "print(\"seaborn version:\", sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions which will be used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the folder path where the PDF should be saved\n",
    "output_folder = r\"C:\\Users\\Lenovo\\ML_Tutorial_Python\\UpGrad AIML Asignments\\Linear-Regression-main\\Assignment\\Delivery_Starter\"\n",
    "\n",
    "# Ensure the folder exists (optional, in case you want to create it if missing)\n",
    "#os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define the full path for the PDF file\n",
    "pdf_filename = os.path.join(output_folder, \"LR_Delivery_Time_Analysis_Plots_Susom_Bikash_Mukherjee.pdf\")\n",
    "\n",
    "# Create the PdfPages object to save multiple plots\n",
    "pdf_pages = PdfPages(pdf_filename)\n",
    "\n",
    "def save_plot_to_pdf(comment=None):\n",
    "    global pdf_pages\n",
    "    \n",
    "    if comment:\n",
    "        print(f\" {comment}\")  # Print the comment for clarity before saving\n",
    "        \n",
    "    pdf_pages.savefig()  # Save the current figure to the PDF\n",
    "    print(\"Plot saved to PDF!\")\n",
    "    \n",
    "# Call pdf_pages.close() when all plots are added to save the file properly\n",
    "def finalize_pdf():\n",
    "    global pdf_pages\n",
    "    pdf_pages.close()\n",
    "    print(f\" PDF file '{pdf_filename}' has been created successfully!\")\n",
    "\n",
    "\n",
    "def save_text_to_pdf(text):\n",
    "    # Function to save a text-based conclusion/remark as a separate page in the PDF\n",
    "    global pdf_pages\n",
    "\n",
    "    # Create a blank figure for text\n",
    "    fig, ax = plt.subplots(figsize=(10, 3))  # Standard A4 page size\n",
    "    ax.set_axis_off()  # Hide axes\n",
    "    \n",
    "    # Display the text\n",
    "    ax.text(0.1, 0.9, \"Conclusion/Remarks:\", fontsize=16, fontweight=\"bold\", transform=ax.transAxes)\n",
    "    ax.text(0.1, 0.75, text, fontsize=12, transform=ax.transAxes, verticalalignment=\"top\", wrap=True)\n",
    "\n",
    "    # Save the figure with text to PDF\n",
    "    pdf_pages.savefig(fig)\n",
    "    print(\"Text remark saved to PDF!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MueJxkvUIII3"
   },
   "source": [
    "## **1. Loading the data**\n",
    "Load 'porter_data_1.csv' as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VJS8ZRJXHTwv"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'porter_data_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Importing the file porter_data_1.csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df_del_time \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mporter_data_1.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check the head of the dataset\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df_del_time\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'porter_data_1.csv'"
     ]
    }
   ],
   "source": [
    "# Importing the file porter_data_1.csv\n",
    "df_del_time = pd.read_csv(\"porter_data_1.csv\")\n",
    "\n",
    "# Check the head of the dataset\n",
    "df_del_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSRQocOkMSQl"
   },
   "source": [
    "## **2. Data Preprocessing and Feature Engineering** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02uPO8aQfLnn"
   },
   "source": [
    "#### **2.1 Fixing the Datatypes**  <font color = red>[5 marks]</font> <br>\n",
    "The current timestamps are in object format and need conversion to datetime format for easier handling and intended functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b22Kzjew3rdM"
   },
   "source": [
    "##### **2.1.1** <font color = red>[2 marks]</font> <br>\n",
    "Convert date and time fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoGkz909IXjv"
   },
   "outputs": [],
   "source": [
    "# Convert 'created_at' and 'actual_delivery_time' columns to datetime format\n",
    "\n",
    "# making sure it doesn't fail even if there are bad/malformed dates, and it will set invalid parses to NaT (Not a Time)\n",
    "df_del_time['created_at'] = pd.to_datetime(df_del_time['created_at'], errors='coerce')\n",
    "df_del_time['actual_delivery_time'] = pd.to_datetime(df_del_time['actual_delivery_time'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data type\n",
    "df_del_time.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1EBPjFc4Qca"
   },
   "source": [
    "##### **2.1.2**  <font color = red>[3 marks]</font> <br>\n",
    "Convert categorical fields to appropriate data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PihPSPhQq1nQ"
   },
   "outputs": [],
   "source": [
    "# Convert categorical features to category type\n",
    "\n",
    "print( df_del_time[\"market_id\"].nunique() )\n",
    "print( df_del_time[\"order_protocol\"].nunique() )\n",
    "print( df_del_time[\"store_primary_category\"].nunique())\n",
    "print( df_del_time[\"total_items\"].nunique())\n",
    "print( df_del_time[\"num_distinct_items\"].nunique())\n",
    "\n",
    "# Convert categorical columns to 'category' dtype\n",
    "df_del_time['market_id'] = df_del_time['market_id'].astype('category')\n",
    "df_del_time['store_primary_category'] = df_del_time['store_primary_category'].astype('category')\n",
    "df_del_time['order_protocol'] = df_del_time['order_protocol'].astype('category')\n",
    "\n",
    "#df_del_time['distance'] = df_del_time['distance'].astype(int)\n",
    "\n",
    "\n",
    "df_del_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsEGroRFlX8z"
   },
   "source": [
    "#### **2.2 Feature Engineering** <font color = red>[5 marks]</font> <br>\n",
    "Calculate the time taken to execute the delivery as well as extract the hour and day at which the order was placed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BubGzQyJpHLQ"
   },
   "source": [
    "##### **2.2.1** <font color = red>[2 marks]</font> <br>\n",
    "Calculate the time taken using the features `actual_delivery_time` and `created_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBGS4PZJMciZ"
   },
   "outputs": [],
   "source": [
    "# Calculate time taken in minutes\n",
    "df_del_time['time_taken'] = df_del_time['actual_delivery_time'] - df_del_time['created_at'] \n",
    "\n",
    "# Convert to total seconds\n",
    "#df_del_time['time_taken'] = df_del_time['time_taken'].dt.total_seconds()\n",
    "\n",
    "# Then format back into HH:MM:SS\n",
    "#df_del_time['time_taken'] = pd.to_datetime(df_del_time['time_taken'], unit='s').dt.strftime('%H:%M:%S')\n",
    "df_del_time['time_taken_minutes'] = df_del_time['time_taken'].dt.total_seconds()/ 60   # Converting into minute\n",
    "df_del_time['time_taken_minutes'] = df_del_time['time_taken_minutes'].astype(int)       # I don't want decimal\n",
    "\n",
    "df_del_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngUUAf3XOPAP"
   },
   "source": [
    "##### **2.2.2** <font color = red>[3 marks]</font> <br>\n",
    "Extract the hour at which the order was placed and which day of the week it was. Drop the unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iwA4O5VtNxQW"
   },
   "outputs": [],
   "source": [
    "##################################################################\n",
    "# Extract the hour and day of week from the 'created_at' timestamp\n",
    "##################################################################\n",
    "# 1. Extract weekday name\n",
    "df_del_time['day_of_week'] = df_del_time['created_at'].dt.day_name()\n",
    "\n",
    "# 2. Extract time in 24-hour format (HH:MM)\n",
    "\n",
    "# Step 1: If needed, format as string first\n",
    "df_del_time['created_at_time'] = df_del_time['created_at'].dt.strftime('%H:%M')\n",
    "\n",
    "# Step 2: Convert string back to datetime64[ns], using a dummy date (like today's date)\n",
    "df_del_time['created_at_time'] = pd.to_datetime(df_del_time['created_at_time'], format='%H:%M')\n",
    "\n",
    "\n",
    "########################################## May be used in analysis later on ################################\n",
    "# 3. Extract year (like 2025)\n",
    "df_del_time['year'] = df_del_time['created_at'].dt.year\n",
    "\n",
    "\n",
    "# 4. Extract month name (like 'January', 'February', etc.)\n",
    "#df_del_time['month'] = df_del_time['created_at'].dt.month_name()  # it will return month name so not using\n",
    "df_del_time['month'] = df_del_time['created_at'].dt.month          # it will return month as 1 to 12\n",
    "\n",
    "\n",
    "# 5. Extract just the date part (YYYY-MM-DD)\n",
    "df_del_time['date'] = df_del_time['created_at'].dt.day\n",
    "\n",
    "############################################\n",
    "# Create a categorical feature 'isWeekend'\n",
    "############################################\n",
    "df_del_time['isWeekend'] = df_del_time['day_of_week'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "# Chanding the Day name to number\n",
    "# Create a mapping dictionary\n",
    "day_to_num = {\n",
    "    'Sunday': 0,\n",
    "    'Monday': 1,\n",
    "    'Tuesday': 2,\n",
    "    'Wednesday': 3,\n",
    "    'Thursday': 4,\n",
    "    'Friday': 5,\n",
    "    'Saturday': 6\n",
    "}\n",
    "\n",
    "# Replace day_of_week using the map\n",
    "df_del_time['day_of_week'] = df_del_time['day_of_week'].map(day_to_num)\n",
    "\n",
    "\n",
    "df_del_time.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding columns for better analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column: Hour bin (1 to 24)\n",
    "df_del_time['created_hr_within'] = df_del_time['created_at_time'].dt.hour + 1\n",
    "df_del_time['100X_distance'] = (df_del_time['distance'] * 100).astype(int)\n",
    "\n",
    "df_del_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying One Hot Encoding to Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( df_del_time[\"market_id\"].nunique() )\n",
    "print( df_del_time[\"order_protocol\"].nunique() )\n",
    "print( df_del_time[\"store_primary_category\"].nunique())\n",
    "print( df_del_time[\"day_of_week\"].nunique())\n",
    "print( df_del_time[\"isWeekend\"].nunique())\n",
    "print( df_del_time[\"total_items\"].nunique())\n",
    "print( df_del_time[\"num_distinct_items\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Columns to one-hot encode\n",
    "#cat_cols = ['market_id', 'order_protocol', 'store_primary_category', 'day_of_week', 'isWeekend', 'total_items', 'num_distinct_items']\n",
    "cat_cols = ['market_id', 'order_protocol', 'day_of_week', 'isWeekend']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df_del_time_encoded = pd.get_dummies(df_del_time, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# Identify only the newly created one-hot columns (they contain category indicators)\n",
    "one_hot_cols = df_del_time_encoded.columns.difference(df_del_time.columns)\n",
    "\n",
    "# Convert those columns to integers (0 or 1)\n",
    "df_del_time_encoded[one_hot_cols] = df_del_time_encoded[one_hot_cols].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del_time_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgzSO8wyOTbP"
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "drop_columns = ['created_at' ,'actual_delivery_time','time_taken','created_at_time','distance','created_at_time','year','time_taken' ]  # Not removing day_of_week right now, will do some analysis.\n",
    "\n",
    "\n",
    "# dropping and updating the data frame\n",
    "\n",
    "#df_del_time = df_del_time.drop(columns=drop_columns)\n",
    "df_del_time_encoded.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "df_del_time.drop(columns=drop_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JJxTsQOFKyl"
   },
   "source": [
    "#### **2.3 Creating training and validation sets** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuyPJMpCFyUL"
   },
   "source": [
    "##### **2.3.1** <font color = red>[2 marks]</font> <br>\n",
    " Define target and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVyKFLXTFKRE"
   },
   "outputs": [],
   "source": [
    "# Define target variable (y) and features (X)\n",
    "\n",
    "# 1. DataFrame without 'time_taken'\n",
    "X_df_del_time = df_del_time_encoded.drop(columns=['time_taken_minutes'])\n",
    "\n",
    "# 2. DataFrame with only 'time_taken'\n",
    "y_df_del_time = df_del_time_encoded[['time_taken_minutes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df_del_time.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df_del_time.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e56iVNqdF3G8"
   },
   "source": [
    "##### **2.3.2** <font color = red>[3 marks]</font> <br>\n",
    " Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0t7XtNDEF6Pu"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We specify this so that the train and test data set always have the same rows, respectively\n",
    "X_train, X_test, y_train, y_test, = train_test_split(X_df_del_time, y_df_del_time, test_size= 0.2, random_state = 100)\n",
    "#X_train, X_test = train_test_split(X_df_del_time, test_size= 0.2, random_state = 100)\n",
    "\n",
    "X_train_unscaled = X_train.copy()  # keeting an unscaled copy\n",
    "y_train_unscaled = X_train.copy()  # keeting an unscaled copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQxv96NBAq_y"
   },
   "source": [
    "## **3. Exploratory Data Analysis on Training Data** <font color = red>[20 marks]</font> <br>\n",
    "1. Analyzing the correlation between variables to identify patterns and relationships\n",
    "2. Identifying and addressing outliers to ensure the integrity of the analysis\n",
    "3. Exploring the relationships between variables and examining the distribution of the data for better insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VU1baEcRc1-A"
   },
   "source": [
    "#### **3.1 Feature Distributions** <font color = red> [7 marks]</font> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For This we will use the Data set before Splitting and Encoding i.e df_del_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_del_time.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj7yFI7VJ_va"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "num_vars = ['subtotal', 'min_item_price', 'max_item_price', 'total_onshift_dashers', 'total_busy_dashers',\n",
    "            'total_outstanding_orders','total_items','num_distinct_items',\n",
    "            'store_primary_category','market_id','order_protocol','day_of_week','100X_distance']\n",
    "\n",
    "# Categorical variables = all columns not in num_vars or drop_columns\n",
    "#cat_vars = [col for col in df_del_time.columns if col not in num_vars]\n",
    "cat_vars = [col for col in df_del_time.columns if col not in num_vars + drop_columns]\n",
    "cat_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWMFLWKpHE-R"
   },
   "source": [
    "##### **3.1.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot distributions for numerical columns in the training set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_M0u5G1YR73_"
   },
   "outputs": [],
   "source": [
    "def univariate_plot (df, comment, is_data_scaled='N', num_cols=None, cat_cols=None):\n",
    "    \"\"\"\n",
    "    Plots:\n",
    "    - Histogram for numeric columns if data is scaled\n",
    "    - Countplot for categorical columns if data is not scaled\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame\n",
    "    - comment: text added to the saved plot description\n",
    "    - For numeric : is_data_scaled ='Y' then  numeric data -> kdeplot , 'N' then  numeric data -> histplot\n",
    "    - For categorycal : countplot\n",
    "    - num_cols: list of numeric column names\n",
    "    - cat_cols: list of categorical column names\n",
    "    \"\"\"\n",
    "    if is_data_scaled == 'Y' and num_cols:\n",
    "        for col in num_cols:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            sns.kdeplot(df[col], fill=True)\n",
    "            plt.title(f'KDE Plot of {col}')\n",
    "            plt.tight_layout()\n",
    "            save_plot_to_pdf(f\"{comment} - KDE Plot of {col}\")\n",
    "            plt.show()\n",
    "\n",
    "    elif is_data_scaled == 'N' and num_cols:\n",
    "        for col in num_cols:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            sns.histplot(df[col], bins=30, kde=False)\n",
    "            plt.title(f'Histogram of {col}')\n",
    "            plt.tight_layout()\n",
    "            save_plot_to_pdf(f\"{comment} - Histogram of {col}\")\n",
    "            plt.show()\n",
    "\n",
    "    # Count plots for categorical columns\n",
    "    if cat_cols:\n",
    "        for col in cat_cols:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            sns.countplot(x=col, data=df)\n",
    "            plt.title(f'Countplot of {col}')\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.tight_layout()\n",
    "            save_plot_to_pdf(f\"{comment} - Countplot of {col}\")\n",
    "            plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_del_time.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#univariate_plot(df_del_time,\"3.1.1 Showing distribution for numerical values Only in Histogram,Box and KDE Plot\",'N',num_vars,None)\n",
    "num_vars = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "univariate_plot(X_train,\"3.1.1 Showing distribution for numerical values Only in Histogram,Box and KDE Plot\",'N',num_vars,None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MtpapIvc9rC"
   },
   "source": [
    "##### **3.1.2** <font color = red>[2 marks]</font> <br>\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zr8loNgMLdrm"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "\n",
    "#univariate_plot(df_del_time,\"3.1.2 Showing distribution for numerical values Only in Histogram,Box and KDE Plot\",'N', None,cat_vars)\n",
    "cat_vars = [col for col in X_train.columns if col not in num_vars]\n",
    "univariate_plot(X_train,\"3.1.2 Showing distribution for numerical values Only in Histogram,Box and KDE Plot\",'N', None,cat_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-9pcLxzJZWf"
   },
   "source": [
    "##### **3.1.3** <font color = red>[2 mark]</font> <br>\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(y_train, bins=30, kde=True)   # just y_train, no ['time_taken_minutes']\n",
    "plt.title('Histogram of time_taken_minutes on Training Data')\n",
    "save_plot_to_pdf(\"3.1.3 Histogram of time_taken_minutes on Training Data\")  # Save the plot to PDF\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Week days and Week end distribution of target data (time_taken_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiWe2Bl9R7yL"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "\n",
    "def plot_target_distribution(df, target, comment):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.histplot(\n",
    "        data=df,\n",
    "        x=target,\n",
    "        bins=50,\n",
    "        kde=True,\n",
    "        hue=\"isWeekend\",\n",
    "        multiple=\"dodge\"\n",
    "    )\n",
    "    plt.title(\"Distribution of Delivery Time by Weekend vs Weekday\")\n",
    "    plt.xlabel(\"Time Taken (minutes)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to PDF\n",
    "    save_plot_to_pdf(f\"{comment} - Countplot of time_taken_minutes\")\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_target_distribution(df_del_time,'time_taken_minutes', \"3.1.3 Visualise the distribution of the target variable to understand its spread and any skewness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbxczs61dROZ"
   },
   "source": [
    "#### **3.2 Relationships Between Features** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YH81kNkOOvlx"
   },
   "source": [
    "##### **3.2.1** <font color = red>[3 marks]</font> <br>\n",
    "Scatter plots for important numerical and categorical features to observe how they relate to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIBnRHohR799"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "def bivariate_plot(df, target, comment):\n",
    "    for col in df.columns:\n",
    "        if col != target:\n",
    "            plt.figure(figsize=(8, 4))\n",
    "            sns.scatterplot(x=df[col], y=df[target], alpha=0.5)\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel(target)\n",
    "            plt.title(f'{target} vs {col}')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot to PDF\n",
    "            save_plot_to_pdf(f\"{comment} - Countplot of time_taken_minutes\")\n",
    "            plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivariate_plot(df_del_time,'time_taken_minutes', \"3.2.1 Scatter plots for important numerical and categorical features to observe how they relate to time_taken\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiWL3cKowfZd"
   },
   "outputs": [],
   "source": [
    "# Show the distribution of time_taken for different hours\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def target_distribution_analysis(df,  comment):\n",
    "    # Group by both created_hr_within and isWeekend\n",
    "    avg_time_per_hour = df_del_time.groupby(['created_hr_within', 'isWeekend'])['time_taken_minutes'].mean().reset_index()\n",
    "    \n",
    "    # Sort by hour (optional, just for better x-axis)\n",
    "    avg_time_per_hour = avg_time_per_hour.sort_values('created_hr_within')\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.lineplot(x='created_hr_within', y='time_taken_minutes', hue='isWeekend', data=avg_time_per_hour, marker='o')\n",
    "    plt.title('Average Time Taken per Hour of the Day (Weekend vs Weekday)', fontsize=16)\n",
    "    plt.xlabel('Hour of Order (created_hr_within)', fontsize=14)\n",
    "    plt.ylabel('Average Time Taken (minutes)', fontsize=14)\n",
    "    plt.xticks(range(1, 25))  # 1 to 24\n",
    "    plt.grid(True)\n",
    "        \n",
    "    # Save plot to PDF\n",
    "    save_plot_to_pdf(f\"{comment} \")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution_analysis(df_del_time, \"3.2.1 Show the distribution of time_taken for different hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKg6rBljIJFP"
   },
   "source": [
    "#### **3.3 Correlation Analysis** <font color = red>[5 marks]</font> <br>\n",
    "Check correlations between numerical features to identify which variables are strongly related to `time_taken`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cyk00sbYfnc0"
   },
   "source": [
    "##### **3.3.1** <font color = red>[3 marks]</font> <br>\n",
    "Plot a heatmap to display correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WxrdHdvKR7vy"
   },
   "outputs": [],
   "source": [
    "# Plot the heatmap of the correlation matrix\n",
    "def heatmap_plot(df,  comment):\n",
    "    plt.figure(figsize = (20, 14))\n",
    "    sns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\n",
    "    \n",
    "    # Save plot to PDF\n",
    "    save_plot_to_pdf(f\"{comment} \")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_plot(df_del_time, \"3.3.1 Plot a heatmap to display correlations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df_del_time.corr()\n",
    "\n",
    "# Get the correlation values of 'time_taken_minutes' with all other columns\n",
    "time_taken_corr = correlation_matrix['time_taken_minutes']\n",
    "\n",
    "# Sort the correlations in descending order\n",
    "sorted_corr = time_taken_corr.sort_values(ascending=False)\n",
    "\n",
    "# Print the sorted correlations\n",
    "print(sorted_corr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yuD3RIwffZE"
   },
   "source": [
    "##### **3.3.2** <font color = red>[2 marks]</font> <br>\n",
    "Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MDZN586gH8R_"
   },
   "outputs": [],
   "source": [
    "# Drop 3-5 weakly correlated columns from training dataset\n",
    "#drop_cols_from_train_data = ['store_primary_category','min_item_price','day_of_week','market_id','date','order_protocol']\n",
    "drop_cols_from_train_data = ['store_primary_category','min_item_price','date']\n",
    "\n",
    "\n",
    "# dropping and updating the data frame\n",
    "\n",
    "#df_del_time = df_del_time.drop(columns=drop_columns)\n",
    "df_del_time_encoded.drop(columns=drop_cols_from_train_data, inplace=True)\n",
    "\n",
    "X_train.drop(columns=drop_cols_from_train_data, inplace=True)\n",
    "X_test.drop(columns=drop_cols_from_train_data, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mZv2rz6lxvc"
   },
   "source": [
    "#### **3.4 Handling the Outliers** <font color = red>[5 marks]</font> <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdyAT-OhyH3z"
   },
   "source": [
    "##### **3.4.1** <font color = red>[2 marks]</font> <br>\n",
    "Visualise potential outliers for the target variable and other numerical features using boxplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot for Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ow3Mowo4R71T"
   },
   "outputs": [],
   "source": [
    "# Boxplot for time_taken\n",
    "\n",
    "def box_plot (df, comment,  box_plot_cols):\n",
    "        for col in box_plot_cols:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            sns.boxplot(y=df[col])\n",
    "            plt.title(f'Boxplot of {col}')\n",
    "            plt.tight_layout()\n",
    "            save_plot_to_pdf(f\"{comment} - KDE Plot of {col}\")\n",
    "            plt.show()\n",
    "box_plot_cols = ['100X_distance','subtotal','total_outstanding_orders','num_distinct_items','max_item_price','total_items',\n",
    "                 'total_busy_dashers','total_onshift_dashers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_plot(X_train,\"3.4.1 Visualise potential outliers for the input numerical features using boxplots\",box_plot_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box Plot for Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column]\n",
    "    print(f\"Number of outliers in {column}: {len(outliers)}\")\n",
    "    print(f\"Percentage of outliers: {(len(outliers)/len(df))*100:.2f}%\")\n",
    "    \n",
    "    # Visualize outliers\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot(df[column])\n",
    "    plt.title(f'Boxplot of {column} showing outliers')\n",
    "    \n",
    "    save_plot_to_pdf(\" 3.1.3 Showing outliers in Box Plot Only\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_outliers(y_train, \"time_taken_minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZCaGBKv_stm"
   },
   "source": [
    "##### **3.4.2** <font color = red>[3 marks]</font> <br>\n",
    "Handle outliers present in all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwQ1A_wZ_X_K"
   },
   "outputs": [],
   "source": [
    "# Handle outliers\n",
    "def show_all_outliers(df):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Compute Q1, Q3, and IQR\n",
    "    Q1 = numeric_cols.quantile(0.25)\n",
    "    Q3 = numeric_cols.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier boundaries\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = (numeric_cols < lower_bound) | (numeric_cols > upper_bound)\n",
    "    \n",
    "    # Count of outliers per column\n",
    "    outlier_counts = outliers.sum()\n",
    "    \n",
    "    # Calculate percentage of outliers per column\n",
    "    outlier_percentage = (outlier_counts / len(df)) * 100\n",
    "\n",
    "    # Compute skewness for each column\n",
    "    skewness_values = numeric_cols.skew()\n",
    "    \n",
    "    # Combine count and percentage into a DataFrame\n",
    "    outlier_summary = pd.DataFrame({\n",
    "        \"Outlier Count\": outlier_counts,\n",
    "        \"Outlier Percentage\": outlier_percentage,\n",
    "        \"Skewness\": skewness_values\n",
    "    }).sort_values(by=\"Outlier Percentage\", ascending=False)\n",
    "    \n",
    "    print(\"Outlier summary per column:\\n\", outlier_summary)\n",
    "    \n",
    "    # View rows containing outliers\n",
    "    df_outliers = df[outliers.any(axis=1)]\n",
    "    return df_outliers.head() #, outlier_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_all_outliers(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### removing rows less than 5% outlier (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def drop_or_clip_outliers(df, threshold=5, exclude_cols=None):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "        \n",
    "    # Select numeric columns, excluding specified ones\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=['number']).columns if col not in exclude_cols]\n",
    "    df_numeric = df[numeric_cols]\n",
    "\n",
    "    # Compute IQR\n",
    "    Q1 = df_numeric.quantile(0.25)\n",
    "    Q3 = df_numeric.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    # setting the upper and lower boundary values for each column\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = (df_numeric < lower_bound) | (df_numeric > upper_bound)\n",
    "\n",
    "    # Compute outlier percentage before removal\n",
    "    outlier_percent_before = (outliers.sum() / len(df)) * 100\n",
    "\n",
    "    # Decide which columns to drop rows for and which to clip\n",
    "    # marking rows to drop if the outlier percent is < 5% \n",
    "    cols_to_drop_outliers = outlier_percent_before[outlier_percent_before < threshold].index.tolist()\n",
    "    # marking rows to cap if the outlier percent is >= 5% \n",
    "    cols_to_clip = outlier_percent_before[outlier_percent_before >= threshold].index.tolist()\n",
    "\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # --- Drop rows with outliers in low-outlier columns ---\n",
    "    if cols_to_drop_outliers:\n",
    "        drop_mask = outliers[cols_to_drop_outliers].any(axis=1)\n",
    "        df_processed = df_processed[~drop_mask].copy()\n",
    "\n",
    "    # --- Clip values in high-outlier columns ---\n",
    "    for col in cols_to_clip:\n",
    "        df_processed[col] = df_processed[col].clip(lower=lower_bound[col], upper=upper_bound[col])\n",
    "\n",
    "    # Recalculate outlier percentage and skewness after cleaning\n",
    "    df_numeric_filtered = df_processed[numeric_cols]\n",
    "    outliers_after = (df_numeric_filtered < lower_bound) | (df_numeric_filtered > upper_bound)\n",
    "    outlier_percent_after = (outliers_after.sum() / len(df_processed)) * 100\n",
    "    skewness_after = df_numeric_filtered.skew()\n",
    "\n",
    "    # Create and print summary\n",
    "    summary_df = pd.DataFrame({\n",
    "        \"Outlier % Before\": outlier_percent_before,\n",
    "        \"Rows Before\": len(df),\n",
    "        \"Outlier % After\": outlier_percent_after,\n",
    "        \"Rows After\": len(df_processed),\n",
    "        \"Skewness After\": skewness_after\n",
    "    })\n",
    "\n",
    "    print(\"\\nOutlier Summary (Row Drop + Clipping):\\n\")\n",
    "    print(summary_df.to_string())\n",
    "\n",
    "    return df_processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply function and create new DF df_non_negative_drop_outlier\n",
    "df_train_no_outlier = drop_or_clip_outliers(X_train, threshold=5, exclude_cols=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0Cd2J-LGWaF"
   },
   "source": [
    "## **4. Exploratory Data Analysis on Validation Data** <font color = red>[optional]</font> <br>\n",
    "Optionally, perform EDA on test data to see if the distribution match with the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Zq16lr0Q9IG"
   },
   "source": [
    "#### **4.1 Feature Distributions**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuoIVgXlQC9y"
   },
   "source": [
    "##### **4.1.1**\n",
    "Plot distributions for numerical columns in the validation set to understand their spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sN6bG_hTbUE"
   },
   "outputs": [],
   "source": [
    "# Define numerical and categorical columns for easy EDA and data manipulation\n",
    "\n",
    "def plot_numeric_columns(df,comment):\n",
    "    \"\"\"Plots Histogram, Boxplot, and KDE Plot for all numeric columns in a given DataFrame.\"\"\"\n",
    "    #num_cols = df.select_dtypes(include=[\"number\"]).columns\n",
    "    num_cols = [col for col in df.select_dtypes(include=[\"number\"]).columns if col not in exclude_cols]\n",
    "\n",
    "    for col in num_cols:\n",
    "        plt.figure(figsize=(18, 5))\n",
    "\n",
    "        # Histogram: Data Distribution\n",
    "        plt.subplot(1, 3, 1)\n",
    "        sns.histplot(df[col], bins=30, kde=True)\n",
    "        plt.title(f'Histogram of {col}')\n",
    "\n",
    "        # Boxplot: Quartile range and Outliers\n",
    "        plt.subplot(1, 3, 2)\n",
    "        sns.boxplot(y=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "\n",
    "        # KDE Plot: Probability Distribution\n",
    "        plt.subplot(1, 3, 3)\n",
    "        sns.kdeplot(df[col], fill=True)\n",
    "        plt.title(f'Density Plot of {col}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        save_plot_to_pdf(f\"{comment} For column {col}\")\n",
    "        plt.show()\n",
    "\n",
    "# Catrgorical Columns to exclude from outlier removal\n",
    "exclude_cols = [\"market_id\", \"store_primary_category\", \"order_protocol\", \"day_of_week\", \"year\", \"month\", \"date\", \"weekday\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "\n",
    "# Plot for df_train (Before Handling Outlier)\n",
    "plot_numeric_columns(X_test,\"4.1.1 Showing distribution for numerical values Only in Histogram,Box and KDE Plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKgSvKvzG8fv"
   },
   "outputs": [],
   "source": [
    "# Plot distributions for all numerical columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrywBQGWQC9z"
   },
   "source": [
    "##### **4.1.2**\n",
    "Check the distribution of categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0CIcl2tHBwp"
   },
   "outputs": [],
   "source": [
    "# Distribution of categorical columns\n",
    "def plot_numeric_distribution(df, selected_columns, draw_plot):\n",
    "    \n",
    "    for col in selected_columns:\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(df[col].describe())  # Print basic statistics like mean, std, min, max\n",
    "        \n",
    "        if draw_plot == 'Y':\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 6), gridspec_kw={'width_ratios': [1.2, 1.8]})\n",
    "            \n",
    "            # Boxplot (smaller)\n",
    "            sns.boxplot(x=df[col], palette=\"coolwarm\", ax=axes[0])\n",
    "            axes[0].set_title(f'Boxplot of {col}')\n",
    "            axes[0].set_xlabel(col)\n",
    "            axes[0].set_ylabel(\"Value\")\n",
    "            \n",
    "            # Histogram (bigger)\n",
    "            sns.histplot(df[col], kde=True, color='skyblue', bins=20, ax=axes[1])\n",
    "            axes[1].set_title(f'Histogram of {col}')\n",
    "            axes[1].set_xlabel(col)\n",
    "            axes[1].set_ylabel(\"Frequency\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            save_plot_to_pdf(f\"4.1.2 Showing Categorical values for {col} Only in Boxplot, and Histogram Plot\") # Save the plot to PDF\n",
    "            plt.show()\n",
    "\n",
    "# Get numeric columns dynamically\n",
    "numeric_columns = [col for col in X_test.select_dtypes(include=['number']).columns]\n",
    "\n",
    "# Optionally exclude any specific numeric columns (if you have any to exclude)\n",
    "exclude_columns = ['total_items','subtotal','num_distinct_items','total_onshift_dashers','max_item_price','total_busy_dashers',\n",
    "                  'total_outstanding_orders','100X_distance']  # List columns to exclude here if needed\n",
    "numeric_columns = [col for col in numeric_columns if col not in exclude_columns]\n",
    "\n",
    "print(\"Plots created...\")\n",
    "# Run function with plotting\n",
    "plot_numeric_distribution(X_test, numeric_columns, draw_plot='Y')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_j74bnlQC9z"
   },
   "source": [
    "##### **4.1.3**\n",
    "Visualise the distribution of the target variable to understand its spread and any skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dGfR8MHGtqm"
   },
   "outputs": [],
   "source": [
    "# Distribution of time_taken\n",
    "plt.plot()\n",
    "sns.histplot(y_test['time_taken_minutes'], bins=30, kde=True)\n",
    "plt.title(f'Histogram of time_taken_minutes on Test Data')\n",
    "save_plot_to_pdf(f\"4.1.3 Histogram of time_taken_minutes on Test Data\") # Save the plot to PDF\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki2FI7fsHDgK"
   },
   "source": [
    "#### **4.2 Relationships Between Features**\n",
    "Scatter plots for numerical features to observe how they relate to each other, especially to `time_taken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lzNPoK4HFnZ"
   },
   "outputs": [],
   "source": [
    "# Scatter plot to visualise the relationship between time_taken and other features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8VoM0XfXWko"
   },
   "source": [
    "#### **4.3** Drop the columns with weak correlations with the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BnM8w2lXWkp"
   },
   "outputs": [],
   "source": [
    "# Drop the weakly correlated columns from training dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReNN4PyM8enl"
   },
   "source": [
    "## **5. Model Building** <font color = red>[15 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2l2XfNF6nc8L"
   },
   "source": [
    "#### **Import Necessary Libraries **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__fmfT6vQWpd"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import statsmodels.api as sm  \n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fCLIKw5pQiA7"
   },
   "source": [
    "#### **5.1 Feature Scaling** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_unscaled = X_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "newEgSyyQiHK"
   },
   "outputs": [],
   "source": [
    "# Apply scaling to the numerical columns\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 1. Fit the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# 2. Transform the  for train and test separately\n",
    "#X_train_df = scaler.transform(X_train)\n",
    "#X_test_df = scaler.transform(X_test)\n",
    "\n",
    "X_train_df = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns, index=X_train.index)\n",
    "X_test_df = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXcV5Z_E8tLL"
   },
   "source": [
    "Note that linear regression is agnostic to feature scaling. However, with feature scaling, we get the coefficients to be somewhat on the same scale so that it becomes easier to compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bxip-t3Y1MB"
   },
   "source": [
    "#### **5.2 Build a linear regression model** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7jZciTFtric"
   },
   "source": [
    "You can choose from the libraries *statsmodels* and *scikit-learn* to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By LinearRegression from sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking and selecting Top 10 segnificant features by RFE estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DMRpgx_iQYM4"
   },
   "outputs": [],
   "source": [
    "# Create/Initialise the model\n",
    "# Running RFE with the output number of the variable equal to 10\n",
    "lr_model = LinearRegression()\n",
    "#lm.fit(X_train_df, y_train)\n",
    "\n",
    "#Model is LR and selecting Feature=10\n",
    "rfe_col_selector = RFE(estimator=lr_model, n_features_to_select=10) \n",
    "\n",
    "rfe_col_selector = rfe_col_selector.fit(X_train_df, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train_df.columns,rfe_col_selector.support_,rfe_col_selector.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_col_selector.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only those columns where it is True\n",
    "selected_feature = X_train_df.columns[rfe_col_selector.support_]\n",
    "\n",
    "print(selected_feature)\n",
    "X_train = X_train_df[selected_feature]\n",
    "X_test = X_test_df[selected_feature]\n",
    "\n",
    "print (X_train.shape)\n",
    "print (X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# If y_train is not Series, flatten it\n",
    "if len(y_train.shape) > 1:\n",
    "    y_train = y_train.squeeze()\n",
    "\n",
    "# Create a temporary DataFrame combining X_train and y_train\n",
    "temp_df = X_train.copy()\n",
    "temp_df['time_taken_minutes'] = y_train\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = temp_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Calculate correlation\n",
    "corr = temp_df[numeric_cols].corr()\n",
    "\n",
    "# Plot correlation with the target variable only\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr[['time_taken_minutes']].sort_values(by='time_taken_minutes', ascending=False),\n",
    "            cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "plt.title('Correlationbetween Target and Input Features on Train data', fontsize=18)\n",
    "\n",
    "save_plot_to_pdf(f\" 5.2 Correlationbetween Target and Input Features  on Train data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select only the 3 columns you want\n",
    "cols_to_check = ['total_onshift_dashers', 'total_busy_dashers', 'total_outstanding_orders']\n",
    "df_selected = X_train[cols_to_check]\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = df_selected.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Between Selected Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking only those columns where it is True\n",
    "\n",
    "dropped_feature = ['total_onshift_dashers']\n",
    "\n",
    "print(dropped_feature)\n",
    "X_train = X_train.drop(columns=dropped_feature)\n",
    "X_test = X_test.drop(columns=dropped_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_lr_model = lr_model.predict(X_test)\n",
    "\n",
    "# Calculate R2 and Adjusted R2\n",
    "r2 = r2_score(y_test, y_pred_lr_model)\n",
    "n = X_test.shape[0]\n",
    "p = X_test.shape[1]\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "print(\"\\n--- Scikit-learn Linear Regression ---\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "print(f\"Adjusted R-squared: {adjusted_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X = X_train\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By OLS from statesmodel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_df[selected_feature]\n",
    "X_test = X_test_df[selected_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm  \n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "X_test_sm = sm.add_constant(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbJVZpMiW8b2"
   },
   "outputs": [],
   "source": [
    "# Train the model using the training data\n",
    "lm = sm.OLS(y_train,X_train_sm).fit()   # Running the linear model\n",
    "\n",
    "\n",
    "print(lm.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "X = X_train_sm\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zU8OLQ4bnwdr"
   },
   "source": [
    "#### **5.3 Build the model and fit RFE to select the most important features** <font color = red>[7 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have 12 (depending on how you select features) training features. However, not all of them would be useful. Let's say we want to take the most relevant 8 features.\n",
    "\n",
    "We will use Recursive Feature Elimination (RFE) here.\n",
    "\n",
    "For this, you can look at the coefficients / p-values of features from the model summary and perform feature elimination, or you can use the RFE module provided with *scikit-learn*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RFE, we will start with all features and use the RFE method to recursively reduce the number of features one-by-one.\n",
    "\n",
    "After analysing the results of these iterations, we select the one that has a good balance between performance and number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Model with statsmodels model = sm.OLS(y_train, X_train).fit()\n",
    "#### Not with sklearn.linear_model.LinearRegression model = LinearRegression().fit(X_train, y_train),  (model.coef_, model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the number of features and test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = X_train.drop([\"total_onshift_dashers\"], axis = 1)\n",
    "#X_test = X_test.drop([\"total_onshift_dashers\"], axis = 1)\n",
    "\n",
    "X_train_1 = X_train_sm.drop([\"total_onshift_dashers\"], axis = 1)\n",
    "X_test_1 = X_test_sm.drop([\"total_onshift_dashers\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_test.shape)\n",
    "#print(X_train.shape)\n",
    "\n",
    "print(X_test_1.shape)\n",
    "print(X_train_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = sm.add_constant(X_train_1)\n",
    "X_test_1 = sm.add_constant(X_test_1)\n",
    "\n",
    "#lm1 = sm.OLS(y_train,X_train).fit()   # Running the linear model\n",
    "lm1 = sm.OLS(y_train,X_train_1).fit()   # Running the linear model\n",
    "print(lm1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "#X = X_train\n",
    "X = X_train_1\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = X_train_1.drop([\"total_busy_dashers\"], axis = 1)\n",
    "X_test_2 = X_test_1.drop([\"total_busy_dashers\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_2.shape)\n",
    "print(X_train_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2 = sm.add_constant(X_train_2)\n",
    "X_test_2 = sm.add_constant(X_test_2)\n",
    "\n",
    "lm2 = sm.OLS(y_train,X_train_2).fit()   # Running the linear model\n",
    "print(lm2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "X = X_train_2\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = X_train_2.drop([\"market_id_2\",\"market_id_3\",\"market_id_4\",\"market_id_5\"], axis = 1)\n",
    "X_test_3 = X_train_2.drop([\"market_id_2\",\"market_id_3\",\"market_id_4\",\"market_id_5\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_3 = sm.add_constant(X_train_3)\n",
    "X_test_3 = sm.add_constant(X_test_3)\n",
    "\n",
    "lm3 = sm.OLS(y_train,X_train_3).fit()   # Running the linear model\n",
    "print(lm3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif = pd.DataFrame()\n",
    "X = X_train_3\n",
    "vif['Features'] = X.columns\n",
    "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(y_train.shape) > 1:\n",
    "    y_train = y_train.squeeze()\n",
    "\n",
    "# Create a temporary DataFrame combining X_train and y_train\n",
    "temp_df = X_train.copy()\n",
    "temp_df['time_taken_minutes'] = y_train\n",
    "\n",
    "# Select only numeric columns for correlation\n",
    "numeric_cols = temp_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Calculate correlation\n",
    "corr = temp_df[numeric_cols].corr()\n",
    "\n",
    "# Plot correlation with the target variable only\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr[['time_taken_minutes']].sort_values(by='time_taken_minutes', ascending=False),\n",
    "            cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "plt.title('Correlationbetween Target and Input Features  on Train data', fontsize=18)\n",
    "save_plot_to_pdf(f\"5.3 Correlationbetween Target and Input Features  on Train data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking performance of Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCQcJtDbW_dG"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "# Predict on test data\n",
    "y_train_pred_2 = lm2.predict(X_train_2)\n",
    "y_test_pred_2 = lm2.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Udw5kE1fXBsR"
   },
   "outputs": [],
   "source": [
    "# Find results for evaluation metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"--- Evaluation on {dataset_name} ---\")\n",
    "    print(f\"MAE : {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R²   : {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Now evaluate\n",
    "evaluate_model(y_train, y_train_pred_2, \"Train Data\")\n",
    "evaluate_model(y_test, y_test_pred_2, \"Test Data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking performance of Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_1 = lm1.predict(X_train_1)\n",
    "y_test_pred_1 = lm1.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"--- Evaluation on {dataset_name} ---\")\n",
    "    print(f\"MAE : {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R²   : {r2:.4f}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "# Now evaluate\n",
    "evaluate_model(y_train, y_train_pred_1, \"Train Data\")\n",
    "evaluate_model(y_test, y_test_pred_1, \"Test Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_actual_vs_predicted_dual(y_train, y_train_pred, y_test, y_test_pred, title=\"Actual vs Predicted (Train & Test)\"):\n",
    "    \"\"\"\n",
    "    Plots Actual vs Predicted for both Train and Test datasets on the same graph.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_train: Actual values for training set\n",
    "    - y_train_pred: Predicted values for training set\n",
    "    - y_test: Actual values for test set\n",
    "    - y_test_pred: Predicted values for test set\n",
    "    \"\"\"\n",
    "    # Ensure all arrays are 1D\n",
    "    y_train = y_train.values.ravel() if hasattr(y_train, 'values') else y_train.ravel()\n",
    "    y_train_pred = y_train_pred.ravel()\n",
    "    y_test = y_test.values.ravel() if hasattr(y_test, 'values') else y_test.ravel()\n",
    "    y_test_pred = y_test_pred.ravel()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plots\n",
    "    sns.scatterplot(x=y_train, y=y_train_pred, label=\"Train\", alpha=0.5, color='blue')\n",
    "    sns.scatterplot(x=y_test, y=y_test_pred, label=\"Test\", alpha=0.5, color='green')\n",
    "\n",
    "    # Reference line (ideal prediction)\n",
    "    min_val = min(y_train.min(), y_test.min())\n",
    "    max_val = max(y_train.max(), y_test.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal')\n",
    "\n",
    "    plt.xlabel(\"Actual Delivery Time\")\n",
    "    plt.ylabel(\"Predicted Delivery Time\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    save_plot_to_pdf(f\"{title} 5.3 Actual vs Predicted (Train & Test) \")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted_dual(y_train, y_train_pred_2, y_test, y_test_pred_2, title=\"Model 2 - Actual vs Predicted (Train & Test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_actual_vs_predicted_dual(y_train, y_train_pred_1, y_test, y_test_pred_1, title=\"Model 1 - Actual vs Predicted (Train & Test)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7p-CAQn3wQE"
   },
   "outputs": [],
   "source": [
    "# Build the final model with selected number of features\n",
    "\n",
    "# Considering lm2 (model 2) is the best amont above 3 models\n",
    "X_train_final = sm.add_constant(X_train_2)\n",
    "X_test_final = sm.add_constant(X_test_2)\n",
    "\n",
    "sm_final = sm.OLS(y_train,X_train_2).fit()   # Running the linear model\n",
    "print(sm_final.summary())\n",
    "\n",
    "# Predict on test data\n",
    "y_train_pred_final_sm = sm_final.predict(X_train_2)\n",
    "y_test_pred_final_sm = sm_final.predict(X_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model 1 has a higher R-squared and Adjusted R-squared, meaning it explains more of the variance in the dependent variable, but it shows signs of multicollinearity (with VIFs above 5). This suggests that some of the features in Model 1 might be too closely related, which could reduce the precision of the coefficient estimates.\n",
    "\n",
    "- Model 2 has a lower R-squared, but its VIF values are much lower, which means it's likely more stable and less affected by multicollinearity. While it doesn't fit the data as well as Model 1, it could be a better option for generalization in cases where multicollinearity is a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1 is giving me 75% result result though the VIF value for total_outstanding_orders is more than 7 \n",
    "#### Where as Model 2 59% result but all VIF values are < 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0l_mLL_4OOl"
   },
   "source": [
    "## **6. Results and Inference** <font color = red>[5 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsPGaacJ71mt"
   },
   "source": [
    "#### **6.1 Perform Residual Analysis** <font color = red>[3 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of Residuals - On Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lbj7O8rf7SZS"
   },
   "outputs": [],
   "source": [
    "# Perform residual analysis using plots like residuals vs predicted values, Q-Q plot and residual histogram\n",
    "\n",
    "res_train_final = (y_train - y_train_pred_final_sm)\n",
    "res_train_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "sns.distplot(res_train_final, bins = 15)\n",
    "fig.suptitle('Error Terms', fontsize = 15)                  # Plot heading \n",
    "plt.xlabel('y_train - res_train_final', fontsize = 15)         # X-label\n",
    "save_plot_to_pdf(f\" 6.1 Residual Analysis on Train data of Model 1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predict on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "# Calculate residuals\n",
    "# Reset index if needed\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "# Make sure both are 1D arrays\n",
    "y_test_reset = y_test_reset.values.ravel()   # Convert to 1D numpy array\n",
    "y_test_pred_final = y_test_pred_final_sm.ravel()        \n",
    "\n",
    "# Now safe to subtract\n",
    "res_test_final = y_test_reset - y_test_pred_final_sm\n",
    "\n",
    "res_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_test_pred_final_sm, y=res_test_final)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted Values - - On Test Data of Final Model')\n",
    "save_plot_to_pdf(f\" 6.1 Residuals vs Predicted Values - On Test Data of Final Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q-Q Plot (Quantile-Quantile Plot) On Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "stats.probplot(res_test_final, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals - On Test Data of Final Model')\n",
    "save_plot_to_pdf(f\" 6.1 Q-Q Plot (Quantile-Quantile Plot) - On Test Data of Final Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of Residuals - On Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(res_test_final, kde=True, bins=30)\n",
    "plt.xlabel('Residuals')\n",
    "plt.title('Histogram of Residuals - On Test Data of Final Model')\n",
    "save_plot_to_pdf(f\" 6.1 Histogram of Residuals - On Test Data of Final Model\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq4g9xPsu4T5"
   },
   "source": [
    "[Your inferences here:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2-CiCId7_y9"
   },
   "source": [
    "#### **6.2 Perform Coefficient Analysis** <font color = red>[2 marks]</font> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2koFJovu-cH"
   },
   "source": [
    "Perform coefficient analysis to find how changes in features affect the target.\n",
    "Also, the features were scaled, so interpret the scaled and unscaled coefficients to understand the impact of feature changes on delivery time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train_2.columns))   # Number of features (should be N)\n",
    "print(len(sm_final.params)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Get Scaled Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_coeffs = pd.DataFrame({\n",
    "    \"Feature\": sm_final.params.index[1:],  # Skip intercept\n",
    "    \"Scaled Coefficient\": sm_final.params.values[1:]  # Skip intercept\n",
    "})\n",
    "scaled_coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Compute Unscaled Coefficients: Unscaled Coef = Scaled Coef × (std_y / std_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sr8EWhg_9QnI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Get the scaled coefficients\n",
    "scaled_coefficients = sm_final.params  # Series with feature names as index\n",
    "\n",
    "# 2. Exclude 'const' from feature list\n",
    "features = scaled_coefficients.index.drop('const')\n",
    "\n",
    "# 3. Get the standard deviations — only for the real features\n",
    "std_devs = X_df_del_time[features].std(axis=0)\n",
    "\n",
    "# 4. Calculate unscaled coefficients (skip const)\n",
    "unscaled_coefficients = scaled_coefficients[features] * std_devs\n",
    "\n",
    "# 5. Create a DataFrame to display\n",
    "coeff_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Scaled Coefficient\": scaled_coefficients[features].values,\n",
    "    \"Standard Deviation (Original)\": std_devs.values,\n",
    "    \"Unscaled Coefficient\": unscaled_coefficients.values\n",
    "})\n",
    "\n",
    "# Show the result\n",
    "coeff_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQ5VcQ2G-SOb"
   },
   "source": [
    "Additionally, we can analyse the effect of a unit change in a feature. In other words, because we have scaled the features, a unit change in the features will not translate directly to the model. Use scaled and unscaled coefficients to find how will a unit change in a feature affect the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=coeff_df, \n",
    "    x=\"Scaled Coefficient\", \n",
    "    y=\"Feature\", \n",
    "    palette=\"coolwarm\"\n",
    ")\n",
    "plt.title(\"Impact of Features on Delivery Time (Scaled Coefficients)\")\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMHN7r-x-Lp5"
   },
   "outputs": [],
   "source": [
    "# Analyze the effect of a unit change in a feature, say 'total_items'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Make sure coeff_df exists from previous steps\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plotting\n",
    "sns.lineplot(\n",
    "    x=coeff_df[\"Feature\"],\n",
    "    y=coeff_df[\"Unscaled Coefficient\"],\n",
    "    marker='o'\n",
    ")\n",
    "\n",
    "# Add horizontal line at y=0 for reference\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "\n",
    "plt.title(\"Impact of Unit Change in Features (Unscaled Coefficients)\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Impact on Target (minutes)\")\n",
    "plt.xticks(rotation=45, ha='right')  # rotate x-axis labels\n",
    "plt.tight_layout()\n",
    "\n",
    "save_plot_to_pdf(f\" 6.2 analyse the effect of a unit change in a feature.\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalize_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFWJ2s9I_Yeo"
   },
   "source": [
    "Note:\n",
    "The coefficients on the original scale might differ greatly in magnitude from the scaled coefficients, but they both describe the same relationships between variables.\n",
    "\n",
    "Interpretation is key: Focus on the direction and magnitude of the coefficients on the original scale to understand the impact of each variable on the response variable in the original units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClCit1tvKIyE"
   },
   "source": [
    "Include conclusions in your report document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mn-wDgoeSiHP"
   },
   "source": [
    "## Subjective Questions <font color = red>[20 marks]</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions only in the notebook. Include the visualisations/methodologies/insights/outcomes from all the above steps in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVJSi-Q0Cw_r"
   },
   "source": [
    "#### Subjective Questions based on Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_jiT95xTA6q"
   },
   "source": [
    "##### **Question 1.** <font color = red>[2 marks]</font> <br>\n",
    "\n",
    "Are there any categorical variables in the data? From your analysis of the categorical variables from the dataset, what could you infer about their effect on the dependent variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvFQvBy3VM9A"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqPxxtWEY3_W"
   },
   "source": [
    "1. isWeekend\n",
    "From your coefficient analysis, it has the strongest positive impact on delivery time.\n",
    "\n",
    "Inference: Deliveries on weekends are significantly slower, likely due to higher order volume or reduced driver availability.\n",
    "\n",
    "2. order_protocol\n",
    "Has a negative coefficient, meaning some protocols (like calls or in-app orders) reduce delivery time.\n",
    "\n",
    "Inference: The way the order is placed impacts efficiency — perhaps in-app orders are quicker to process.\n",
    "\n",
    "3. store_primary_category\n",
    "Your earlier boxplots likely showed variation across categories (e.g., fast food might be quicker than full-service dining).\n",
    "\n",
    "Inference: Type of restaurant impacts preparation time and thus total delivery time.\n",
    "\n",
    "4. market_id\n",
    "Geographic location could affect delivery dynamics — cities with heavy traffic or fewer dashers may lead to delays.\n",
    "\n",
    "Inference: Some markets are consistently faster or slower, suggesting logistical or regional challenges.\n",
    "\n",
    "5. day_of_week\n",
    "Often, certain weekdays (like Fridays or Mondays) show trends — possibly longer times before/after weekends.\n",
    "\n",
    "Inference: Delivery time fluctuates by weekday — useful for resource planning.\n",
    "\n",
    "---\n",
    "Inference:\n",
    "One-hot encode variables like store_primary_category or market_id before modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DDSRymTJTHCW"
   },
   "source": [
    "##### **Question 2.** <font color = red>[1 marks]</font> <br>\n",
    "What does `test_size = 0.2` refer to during splitting the data into training and test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRBCcZvoVx-r"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_afbTV8Y5-F"
   },
   "source": [
    "##### 0.2 means that 20% of the total dataset will be allocated to the test set, and the remaining 80% will be used for the training set.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEVX57VbTJBP"
   },
   "source": [
    "##### **Question 3.** <font color = red>[1 marks]</font> <br>\n",
    "Looking at the heatmap, which one has the highest correlation with the target variable?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewPqz4yLWBzR"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLy_-8F5Y69c"
   },
   "source": [
    "Feature Correlation Interpretation subtotal 0.41 Larger orders (in terms of price) tend to take more time, possibly due to prep time. total_outstanding_orders 0.38 More pending orders at the time of ordering → longer delivery times. num_distinct_items 0.31 More variety in the order might lead to more prep complexity. max_item_price 0.25 May reflect premium or time-consuming items.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lg-6E-N-TKyS"
   },
   "source": [
    "##### **Question 4.** <font color = red>[2 marks]</font> <br>\n",
    "What was your approach to detect the outliers? How did you address them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPUDtsRGWLZl"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVyJFcT2Y7U8"
   },
   "source": [
    "##### I used IQR clipping to handle outlier with below boundaries. lower_bound = Q1 - 1.5 * IQR upper_bound = Q3 + 1.5 * IQR Rows deleted where outlier is < 5% Capped the rows where it is > 5% rows\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvh9CLFnTMhO"
   },
   "source": [
    "##### **Question 5.** <font color = red>[2 marks]</font> <br>\n",
    "Based on the final model, which are the top 3 features significantly affecting the delivery time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M-DDpZcCWUun"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVCrLjhTY74h"
   },
   "source": [
    "### Feature Coefficients\n",
    "\n",
    "| Feature                   | Coefficient     |\n",
    "|---------------------------|-----------------|\n",
    "| total_outstanding_orders | 13.188538       |\n",
    "| total_busy_dashers       | -10.478694      |\n",
    "| 100X_distance            | 4.266365        |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VBLH_lA5C4jy"
   },
   "source": [
    "#### General Subjective Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MJGDVyiTOyr"
   },
   "source": [
    "##### **Question 6.** <font color = red>[3 marks]</font> <br>\n",
    "Explain the linear regression algorithm in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZc1QX8RW_Pa"
   },
   "source": [
    "**Answer:**\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0MCb30NY8UE"
   },
   "source": [
    "Linear regression is a supervised learning algorithm used for predicting a continuous target variable (Feature) based on one or more input features. It models the linear relationship between the dependent variable (target) and one or more independent variables (features).\n",
    "The targer value more or less predicted by linear equation with getting the coefficient and intercept value after training the model by train data.\n",
    "\n",
    "There are 2 types of linear regression\n",
    "1. Simple Linear Regression – One independent variable\n",
    "2. Multiple Linear Regression – Multiple independent variables\n",
    "\n",
    "We need to do some assumptions\n",
    "- Linearity – Relationship between X and y is linear.\n",
    "- Independence – Observations are independent.\n",
    "- Homoscedasticity – Constant variance of residuals.\n",
    "- Normality of Residuals – Residuals should be normally distributed.\n",
    "- No Multicollinearity – Features shouldn't be highly correlated with each other.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "db_7gqf8TQTk"
   },
   "source": [
    "##### **Question 7.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between simple linear regression and multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1jsR8htXD8j"
   },
   "source": [
    "**Answer:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FnSGZEltY8ss"
   },
   "source": [
    "\n",
    "Aspect\t                                          Simple Linear Regression\t                Multiple Linear Regression\n",
    "Number of Features\t                                     One\t                                    Two or more\n",
    "Complexity\t                                        Less complex\t                                More complex\n",
    "Visual Representation\t                         Line on a 2D plot\t               Plane or hyperplane in multidimensional space\n",
    "Interpretation\t                                         Easy\t                    Requires interpretation of multiple effects\n",
    "Risk of Multicollinearity\t                        Not applicable\t                               Can be an issue\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT6ivEEnTSEs"
   },
   "source": [
    "##### **Question 8.** <font color = red>[2 marks]</font> <br>\n",
    "What is the role of the cost function in linear regression, and how is it minimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2PaCL-FXSSn"
   },
   "source": [
    "**Answer:**\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIKB_W0FY9QM"
   },
   "source": [
    "In Linear Regression, the most commonly used cost function is: MSE (Mean Square Error)\n",
    ">\n",
    ">We minimize the cost function to find the optimal model parameters (intercept and coefficients). This is typically done in two ways:\n",
    ">Analytical Method (Normal Equation)\n",
    ">Optimization Method (Gradient Descent)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZIb5hbMCCVY"
   },
   "source": [
    "##### **Question 9.** <font color = red>[2 marks]</font> <br>\n",
    "Explain the difference between overfitting and underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8kn4c-7CEjP"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PWIs-suCMEr"
   },
   "source": [
    "**Answer:**\n",
    "\n",
    "Underfitting vs Overfitting\n",
    "\n",
    "| Aspect          | Underfitting                                      | Overfitting                                             |\n",
    "|-----------------|---------------------------------------------------|---------------------------------------------------------|\n",
    "| **Definition**  | Model is too simple to capture patterns           | Model is too complex, captures noise                    |\n",
    "| **Training error** | High                                           | Very low                                                |\n",
    "| **Testing error**  | High                                           | High                                                    |\n",
    "| **Bias**        | High                                              | Low                                                     |\n",
    "| **Variance**    | Low                                               | High                                                    |\n",
    "| **Example**     | Linear model on non-linear data                   | High-degree polynomial on simple data                   |\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7JPKHwArn7"
   },
   "source": [
    "##### **Question 10.** <font color = red>[3 marks]</font> <br>\n",
    "How do residual plots help in diagnosing a linear regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqxU8GSkAubl"
   },
   "source": [
    "**Answer:**\n",
    ">\n",
    ">>A residual is the difference between the actual and predicted value:  Yactual - Y predicted\n",
    ">A residual plot shows these residuals on the y-axis and the predicted values (or an independent variable) on the x-axis.\n",
    ">\n",
    ">If the linear regression model is appropriate:\n",
    "-- The residuals should be randomly scattered around zero.\n",
    "-- There should be no clear pattern.\n",
    "-- The spread (variance) of residuals should be roughly constant — this is called homoscedasticity."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "MueJxkvUIII3",
    "02uPO8aQfLnn",
    "b22Kzjew3rdM",
    "u1EBPjFc4Qca",
    "-JJxTsQOFKyl",
    "v0Cd2J-LGWaF",
    "fCLIKw5pQiA7",
    "2bxip-t3Y1MB",
    "mn-wDgoeSiHP"
   ],
   "provenance": [
    {
     "file_id": "1qHefVpjLoVZcdohzmYySNZGiPR4wQOFp",
     "timestamp": 1737728120597
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
